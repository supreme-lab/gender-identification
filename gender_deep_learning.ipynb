{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagorbrur/bangla-bert/blob/master/notebook/bangla_bert_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !/usr/bin/python3.9 -m pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "import gensim as gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "E1kxhRhAwA8R",
        "outputId": "778edbf9-5fb2-47dc-e097-cdbeec7bb209"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ল্যাবে গ্রুপমেটের গ্রাফ নিজে একে দিয়েছি অনেক ...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>এরকম এক রমজান মাসের স্নিগ্ধ বিকেলে, জনৈক বুয়েট...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>শিবিরবিহীন বুয়েট থেকে শিবিরের কেন্দ্রীয় কমিটির...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>জনৈক বন্ধু তার বান্ধবীর সাথে দেখা করার আগে নিজ...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>খুব ক্লোজ জুনিয়রের সাথে শেরে বাংলা হলের সামনের...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text gender\n",
              "0  ল্যাবে গ্রুপমেটের গ্রাফ নিজে একে দিয়েছি অনেক ...      M\n",
              "1  এরকম এক রমজান মাসের স্নিগ্ধ বিকেলে, জনৈক বুয়েট...      M\n",
              "2  শিবিরবিহীন বুয়েট থেকে শিবিরের কেন্দ্রীয় কমিটির...      M\n",
              "3  জনৈক বন্ধু তার বান্ধবীর সাথে দেখা করার আগে নিজ...      M\n",
              "4  খুব ক্লোজ জুনিয়রের সাথে শেরে বাংলা হলের সামনের...      M"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "dataset_url = 'https://raw.githubusercontent.com/supreme-lab/gender-identification/main/dataset.csv'\n",
        "\n",
        "df = pd.read_csv(dataset_url)\n",
        "df = df.dropna(subset=['text', 'gender'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "colab_type": "code",
        "id": "VirFZSTQZY91",
        "outputId": "49163942-03a0-4603-f8ad-a4e1f182b05c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15367"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.index.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NWRXBgz_aIFU"
      },
      "outputs": [],
      "source": [
        "def get_male_female_keywords(posts, genders):\n",
        "  # compute keyword for male and female samples\n",
        "  male_keywords = []\n",
        "  female_keywords = []\n",
        "  for idx, post in enumerate(posts):\n",
        "    splitted = post.split()\n",
        "    if genders[idx] == \"M\":\n",
        "      male_keywords.extend(splitted)\n",
        "    else:\n",
        "      female_keywords.extend(splitted)\n",
        "  return male_keywords, female_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_frequency_from_list(keywords):\n",
        "  freq = {}\n",
        "  for word in keywords:\n",
        "    if word in freq:\n",
        "      freq[word] = freq[word] + 1\n",
        "    else:\n",
        "      freq[word] = 1\n",
        "  return freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import random\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "class DataPreProcessor(object):\n",
        "  def __init__(self, dataset_url):\n",
        "    self.dataset_url = dataset_url\n",
        "\n",
        "  def preprocess(self):\n",
        "    # stop word list\n",
        "    stop_word_file = 'https://raw.githubusercontent.com/supreme-lab/gender-identification/main/stopwords-bn.txt'\n",
        "    sw_df = pd.read_csv(stop_word_file, delimiter=\"\\t\", header=None)\n",
        "    stopword_list = sw_df.values.tolist()\n",
        "\n",
        "    # load the training data\n",
        "    df = pd.read_csv(self.dataset_url)\n",
        "\n",
        "    # preprocess step 0: remove duplicate rows\n",
        "    df = df.drop_duplicates()\n",
        "    print(\"after removing duplicate rows, row count: \", df.index.size)\n",
        "\n",
        "    # preprocess step 1: remove null rows\n",
        "    df = df.dropna(subset=['text', 'gender'])\n",
        "    print(\"after removing null rows, row count: \", df.index.size)\n",
        "\n",
        "    # preprocess step 2: remove non Bangla characters\n",
        "    author_posts = []\n",
        "    author_genders = []\n",
        "    for index, row in df.iterrows():\n",
        "        tokens = [re.sub(r'[^\\u0980-\\u09FF ]+', ' ', str(row['text']))]\n",
        "        tokens_joined = \" \".join(tokens)\n",
        "        if len(tokens_joined) == 0:\n",
        "          print(\"tokens_joined.len==0\")\n",
        "        author_posts.append(tokens_joined)\n",
        "        author_genders.append(row['gender'])\n",
        "\n",
        "    stopword_list_new = []\n",
        "    for word in stopword_list:\n",
        "      stopword_list_new.append(word[0])\n",
        "    stopword_list = stopword_list_new\n",
        "    # preprocess step 3: remove stop words\n",
        "    author_posts_without_stopword = []\n",
        "    author_genders_without_stopword = []\n",
        "    i = 0\n",
        "    for idx, post in enumerate(author_posts):\n",
        "        i = i + 1\n",
        "        # if i< 100:\n",
        "        #   print(\"before stopword removal sent.len: \", len(post.split()))\n",
        "        tokens_without_stopword = [word for word in post.split() if not word in stopword_list]\n",
        "        # if i<100:\n",
        "        #   print(\"after stopword removal sent.len: \", len(tokens_without_stopword))\n",
        "        tokens_without_stopword_joined = \" \".join(tokens_without_stopword)\n",
        "        if len(tokens_without_stopword_joined) > 0:\n",
        "          # print(\"tokens_without_stopword_joined: \", tokens_without_stopword_joined)\n",
        "          # print(\"len(tokens_without_stopword_joined) == 0\")\n",
        "          author_posts_without_stopword.append(tokens_without_stopword_joined)\n",
        "          author_genders_without_stopword.append(author_genders[idx])\n",
        "    \n",
        "    post_count = len(author_posts_without_stopword)\n",
        "    gender_count = len(author_genders_without_stopword)\n",
        "\n",
        "    print(f\"after stopword removing posts.len: {post_count}, gender.count: {gender_count}\")\n",
        "    print(\"author_genders_without_stopword[0:10]\")\n",
        "    print(author_genders_without_stopword[0:10])\n",
        "\n",
        "    malekeywords, femalekeywords = get_male_female_keywords(author_posts_without_stopword, author_genders)\n",
        "    \n",
        "    # compute keyword occurrence in male and female data\n",
        "    # print(\"compute keyword occurrence in male and female data\")\n",
        "    male_freq = get_frequency_from_list(malekeywords)\n",
        "    female_freq = get_frequency_from_list(femalekeywords)\n",
        "\n",
        "    # filter out keywords with occurrence <10\n",
        "    # print(\"filter out keywords with occurrence <10 \")\n",
        "    features = []\n",
        "    for key, value in male_freq.items():\n",
        "      if value >= 10:\n",
        "        features.append(key)\n",
        "\n",
        "    for key, value in female_freq.items():\n",
        "      if value >= 10:\n",
        "        features.append(key)\n",
        "\n",
        "    # get the unique keywords\n",
        "    # print(\"get the unique keywords\")\n",
        "    featureset = set(features)\n",
        "    # print(\"featureset.len: \", len(featureset))\n",
        "\n",
        "    # keep words which are in the featureset\n",
        "    # print(\"keep words which are in the featureset\")\n",
        "    i = 0\n",
        "    author_posts_with_keywords_in_features = []\n",
        "    author_genders_with_keywords_in_features = []\n",
        "    for idx, post in enumerate(author_posts_without_stopword):\n",
        "        # if i < 100:\n",
        "        #   print(\"post.len: \", len(post.split()))\n",
        "        tokens_filtered_with_features = [word for word in post.split() if word in featureset]\n",
        "        tokens_filtered_with_features_joined = \" \".join(tokens_filtered_with_features)\n",
        "        # if i<100:\n",
        "        #   print(\"tokens_without_keywords_with_occurrence1.len: \", len(tokens_without_keywords_with_occurrence1))\n",
        "        \n",
        "        if len(tokens_filtered_with_features_joined) > 0:\n",
        "          # print(\"len(tokens_filtered_with_features_joined) == 0\")\n",
        "          author_posts_with_keywords_in_features.append(tokens_filtered_with_features_joined)\n",
        "          author_genders_with_keywords_in_features.append(author_genders_without_stopword[idx])\n",
        "    \n",
        "    post_count = len(author_posts_with_keywords_in_features)\n",
        "    gender_count = len(author_genders_with_keywords_in_features)\n",
        "\n",
        "    print(f\"after removing out of features posts.len: {post_count}, gender.count: {gender_count}\")\n",
        "    print(\"author_genders_with_keywords_in_features[0:10]: \", author_genders_with_keywords_in_features[0:10])\n",
        "\n",
        "    return author_posts_with_keywords_in_features, author_genders_with_keywords_in_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after removing duplicate rows, row count:  12726\n",
            "after removing null rows, row count:  12726\n",
            "after stopword removing posts.len: 10544, gender.count: 10544\n",
            "author_genders_without_stopword[0:10]\n",
            "['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F']\n",
            "after removing out of features posts.len: 10397, gender.count: 10397\n",
            "author_genders_with_keywords_in_features[0:10]:  ['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F']\n"
          ]
        }
      ],
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/supreme-lab/gender-identification/main/dataset.csv\"\n",
        "preprocessor = DataPreProcessor(dataset_url)\n",
        "posts, genders = preprocessor.preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10397, 10397)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(posts), len(genders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learning Models with TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelDLWithTFIDF():\n",
        "  def __init__(self, posts, genders, model, batch, epochs) -> None:\n",
        "    self.batch = batch\n",
        "    self.epochs = epochs\n",
        "    self.author_posts = posts\n",
        "\n",
        "    self.author_genders = genders\n",
        "    self.model=Sequential()\n",
        "    if model == 'LSTM':\n",
        "      self.model.add(LSTM(300, return_sequences=False))\n",
        "    elif model == 'GRU':\n",
        "      self.model.add(GRU(300, return_sequences=False))\n",
        "\n",
        "  def train(self):\n",
        "    labels = np.asarray(pd.get_dummies(self.author_genders))\n",
        "    train_x, test_x, train_y, test_y = model_selection.train_test_split(self.author_posts, labels, test_size=0.3)\n",
        "\n",
        "    male_list = []\n",
        "    female_list = []\n",
        "\n",
        "    self.tfidf_vectorizer = TfidfVectorizer(max_features=600, analyzer='word', ngram_range=(1, 3))\n",
        "    self.tfidf_vectorizer.fit(self.author_posts)\n",
        "\n",
        "    train_x_tfidf = self.tfidf_vectorizer.transform(train_x)\n",
        "    test_x_tfidf = self.tfidf_vectorizer.transform(test_x)\n",
        "    train_x_tfidf_dense = train_x_tfidf.toarray()\n",
        "    test_x_tfidf_dense = test_x_tfidf.toarray()\n",
        "\n",
        "    train_x_tfidf_dense = train_x_tfidf_dense.reshape(train_x_tfidf_dense.shape[0],1,train_x_tfidf_dense.shape[-1])\n",
        "    test_x_tfidf_dense = test_x_tfidf_dense.reshape(test_x_tfidf_dense.shape[0],1,test_x_tfidf_dense.shape[-1])\n",
        "\n",
        "    # compile model\n",
        "    self.model.add(Dense(train_y.shape[1], activation='sigmoid'))\n",
        "    self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    history_model = self.model.fit(train_x_tfidf_dense, train_y, self.batch, self.epochs, validation_split=0.25)\n",
        "    scores = self.model.evaluate(test_x_tfidf_dense, test_y, verbose=0)\n",
        "    author_gender_test_predictions = np.argmax(self.model.predict(test_x_tfidf_dense), axis=1)\n",
        "\n",
        "    author_gender_original_label = np.argmax(test_y, axis=1)\n",
        "    accuracy = accuracy_score(author_gender_original_label, author_gender_test_predictions)\n",
        "    precision = precision_score(author_gender_original_label, author_gender_test_predictions)\n",
        "    recall = recall_score(author_gender_original_label, author_gender_test_predictions)\n",
        "    f1 = f1_score(author_gender_original_label, author_gender_test_predictions)\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learning Models with Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelDLWithWordEmbedding(object):\n",
        "    def __init__(self, cbow=1, cbow_trainable=1, sg=0, sg_trainable=0, data_split=2, batch=32, epochs=5, model_name=\"GRU\"):\n",
        "        self.model = Sequential()\n",
        "        self.cbow = cbow\n",
        "        self.cbow_trainable = cbow_trainable\n",
        "        self.sg = sg\n",
        "        self.sg_trainable = sg_trainable\n",
        "        self.data_split = data_split\n",
        "        self.batch = batch\n",
        "        self.epochs = epochs\n",
        "        self.model_name = model_name\n",
        "\n",
        "\n",
        "    def get_stop_words(self):\n",
        "      stop_word_file = 'https://raw.githubusercontent.com/supreme-lab/gender-identification/main/stopwords-bn.txt'\n",
        "      sw_df = pd.read_csv(stop_word_file, delimiter=\"\\t\", header=None)\n",
        "      return sw_df.values.tolist()\n",
        "\n",
        "    def convert_text_to_word_vector(self, author_post, author_gender):\n",
        "      maxlen = 100\n",
        "      max_words = 1000\n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(author_post)\n",
        "      sequences = tokenizer.texts_to_sequences(author_post)\n",
        "\n",
        "      word_index = tokenizer.word_index\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "      labels = np.asarray(pd.get_dummies(author_gender))\n",
        "      return data, labels\n",
        "\n",
        "    def make_dataset(self, X_data, y_data, n_splits):\n",
        "      X_train_dataset= []\n",
        "      X_test_dataset = []\n",
        "      y_train_dataset=  []\n",
        "      y_test_dataset = []\n",
        "\n",
        "      for train_index, test_index in KFold(n_splits).split(X_data):\n",
        "              X_train, X_test = X_data[train_index], X_data[test_index]\n",
        "              y_train, y_test = y_data[train_index], y_data[test_index]\n",
        "              X_train_dataset.append(X_train)\n",
        "              X_test_dataset.append(X_test)\n",
        "              y_train_dataset.append(y_train)\n",
        "              y_test_dataset.append(y_test)\n",
        "\n",
        "      return X_train_dataset, X_test_dataset, y_train_dataset, y_test_dataset\n",
        "\n",
        "    def get_embedding_vector(self, author_post):\n",
        "      tokenized_lines = []\n",
        "      for single_line in author_post:\n",
        "        tokenized_lines.append(single_line.split())\n",
        "\n",
        "      if self.cbow == 1:\n",
        "        # Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        self.word_model = gensim.models.Word2Vec(tokenized_lines, sg=0, vector_size=300, window=5, min_count=1, workers=4)\n",
        "      elif self.sg == 1:\n",
        "        self.word_model = gensim.models.Word2Vec(tokenized_lines, sg=1, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "      embedding_matrix = np.zeros((len(self.word_model.wv.vectors) + 1, 300))  # 29534*300 --> Total word = 29534 ---> prottek word er vector e 300ta element\n",
        "      for i, vec in enumerate(self.word_model.wv.vectors):\n",
        "        embedding_matrix[i] = vec\n",
        "      return embedding_matrix\n",
        "\n",
        "    def get_model(self, embedding_matrix, author_post_train, author_gender_train):\n",
        "        model = Sequential()  # 1 * 100 ---\n",
        "        if self.cbow_trainable == 1:  # 100 * 300  #input_length = 100\n",
        "            model.add(\n",
        "                Embedding(len(self.word_model.wv.vectors) + 1, 300, input_length=author_post_train.shape[1], weights=[embedding_matrix],\n",
        "                          trainable=True))  # size of embedding matrix is 100*300\n",
        "        elif self.sg_trainable == 1:\n",
        "            model.add(\n",
        "                Embedding(len(self.word_model.wv.vectors) + 1, 300, input_length=author_post_train.shape[1], weights=[embedding_matrix],\n",
        "                          trainable=True))\n",
        "        else:\n",
        "            model.add(\n",
        "                Embedding(len(self.word_model.wv.vectors) + 1, 300, input_length=author_post_train.shape[1], weights=[embedding_matrix],\n",
        "                          trainable=False))\n",
        "\n",
        "        # compile model\n",
        "        if self.model_name == \"GRU\":\n",
        "          model.add(GRU(300, return_sequences=False))\n",
        "        elif self.model_name == \"LSTM\":\n",
        "          model.add(LSTM(300, return_sequences=False))\n",
        "\n",
        "        model.add(Dense(author_gender_train.shape[1], activation=\"softmax\"))\n",
        "        model.summary()\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
        "        return model\n",
        "\n",
        "    def train(self, posts, genders):\n",
        "        author_post, author_gender = posts, genders\n",
        "\n",
        "        data , labels = self.convert_text_to_word_vector(author_post, author_gender) #size is post_number*100\n",
        "        X_train_dataset, X_test_dataset, y_train_dataset, y_test_dataset = self.make_dataset(data, labels, self.data_split)\n",
        "        embedding_matrix = self.get_embedding_vector(author_post) # 29534 * 100 dimension er ekta matrix\n",
        "\n",
        "        acc_per_fold = []\n",
        "        loss_per_fold = []\n",
        "        precision_per_fold = []\n",
        "        recall_per_fold = []\n",
        "        f1_score_per_fold = []\n",
        "\n",
        "        for i in range(len(X_train_dataset)):\n",
        "            author_post_train = X_train_dataset[i] # 400*100\n",
        "            author_gender_train = y_train_dataset[i]\n",
        "            author_post_test = X_test_dataset[i]\n",
        "            author_gender_test = y_test_dataset[i]\n",
        "            #batch, epochs = 50, 100\n",
        "            batch, epochs = self.batch, self.epochs\n",
        "            model = self.get_model(embedding_matrix, author_post_train, author_gender_train)\n",
        "            #csv_logger,tensorboard_callback = self.get_csvlogger_tensorflow_callback()\n",
        "            history_model = model.fit(author_post_train, author_gender_train, batch, epochs,\n",
        "                                       validation_split=0.2)\n",
        "            #, callbacks=[csv_logger, tensorboard_callback])\n",
        "            scores = model.evaluate(author_post_test, author_gender_test, verbose=0)\n",
        "            # acc_per_fold.append(scores[1] * 100)\n",
        "            loss_per_fold.append(scores[0])\n",
        "\n",
        "            author_gender_test_predictions = model.predict(author_post_test)\n",
        "            author_gender_predicted_label = np.argmax(author_gender_test_predictions, axis=1)\n",
        "            author_gender_original_label = np.argmax(author_gender_test, axis=1)\n",
        "\n",
        "            TP = tf.math.count_nonzero(author_gender_predicted_label * author_gender_original_label, dtype=tf.float32)\n",
        "            TN = tf.math.count_nonzero((author_gender_predicted_label - 1) * (author_gender_original_label - 1),dtype=tf.float32)\n",
        "            FP = tf.math.count_nonzero(author_gender_predicted_label * (author_gender_original_label - 1), dtype=tf.float32)\n",
        "            FN = tf.math.count_nonzero((author_gender_predicted_label - 1) * author_gender_original_label, dtype=tf.float32)\n",
        "\n",
        "            # print(\"True positive: \", TP, \"True negative: \", TN, \"False positive: \", FP, \"False negative: \", FN)\n",
        "            print(\"TP, TN, FP, FN\")\n",
        "            print(TP, TN, FP, FN)\n",
        "\n",
        "            accuracy = ( TP + TN ) / (TP + FP +FP +FN)\n",
        "            precision = TP / (TP + FP)\n",
        "            recall = TP / (TP + FN)\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "            accuracy = accuracy_score(author_gender_original_label, author_gender_predicted_label)\n",
        "            precision = precision_score(author_gender_original_label, author_gender_predicted_label)\n",
        "            recall = recall_score(author_gender_original_label, author_gender_predicted_label)\n",
        "            f1 = f1_score(author_gender_original_label, author_gender_predicted_label)\n",
        "\n",
        "            acc_per_fold.append(accuracy)\n",
        "            precision_per_fold.append(precision)\n",
        "            recall_per_fold.append(recall)\n",
        "            f1_score_per_fold.append(f1)\n",
        "\n",
        "        sum_acc = 0\n",
        "        sum_loss = 0\n",
        "        sum_precision = 0\n",
        "        sum_recall = 0\n",
        "        sum_f1_score = 0\n",
        "\n",
        "        for i, acc in enumerate(acc_per_fold):\n",
        "            sum_acc = sum_acc + acc\n",
        "            sum_loss = sum_loss+ loss_per_fold[i]\n",
        "            sum_precision = sum_precision + precision_per_fold[i]\n",
        "            sum_recall = sum_recall + recall_per_fold[i]\n",
        "            sum_f1_score = sum_f1_score + f1_score_per_fold[i]\n",
        "\n",
        "        avg_acc = sum_acc/10.0\n",
        "        avg_precision = sum_precision /10.0\n",
        "        avg_recall = sum_recall/10.0\n",
        "        avg_f1 = sum_f1_score /10.0\n",
        "\n",
        "        return acc_per_fold, precision_per_fold, recall_per_fold, f1_score_per_fold, avg_acc, avg_precision, avg_recall, avg_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to Embedding: {'input_length': 100, 'weights': [array([[-0.24736658,  0.07031919,  0.01407981, ..., -0.27251738,\n         0.13224085,  0.0765908 ],\n       [-0.12291483,  0.08461943,  0.06060226, ..., -0.17144129,\n         0.17412944,  0.02834771],\n       [-0.15936373,  0.06352039,  0.02524688, ..., -0.229536  ,\n         0.12055779,  0.05851828],\n       ...,\n       [ 0.00078043,  0.02895817,  0.01444013, ..., -0.01905065,\n         0.02032049,  0.02145052],\n       [ 0.0083534 ,  0.05115167,  0.02525139, ..., -0.02803847,\n         0.03329531,  0.03750573],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])]}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m \n\u001b[1;32m      7\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 9\u001b[0m acc_per_fold, precision_per_fold, recall_per_fold, f1_score_per_fold, avg_acc, avg_precision, avg_recall, avg_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mModelDLWithWordEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGRU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_per_fold: \u001b[39m\u001b[38;5;124m\"\u001b[39m, acc_per_fold)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_per_fold: \u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_per_fold)\n",
            "Cell \u001b[0;32mIn[20], line 110\u001b[0m, in \u001b[0;36mModelDLWithWordEmbedding.train\u001b[0;34m(self, posts, genders)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#batch, epochs = 50, 100\u001b[39;00m\n\u001b[1;32m    109\u001b[0m batch, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\n\u001b[0;32m--> 110\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor_post_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor_gender_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#csv_logger,tensorboard_callback = self.get_csvlogger_tensorflow_callback()\u001b[39;00m\n\u001b[1;32m    112\u001b[0m history_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(author_post_train, author_gender_train, batch, epochs,\n\u001b[1;32m    113\u001b[0m                            validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
            "Cell \u001b[0;32mIn[20], line 68\u001b[0m, in \u001b[0;36mModelDLWithWordEmbedding.get_model\u001b[0;34m(self, embedding_matrix, author_post_train, author_gender_train)\u001b[0m\n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()  \u001b[38;5;66;03m# 1 * 100 ---\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbow_trainable \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# 100 * 300  #input_length = 100\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m---> 68\u001b[0m         \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthor_post_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# size of embedding matrix is 100*300\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msg_trainable \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     72\u001b[0m         Embedding(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m300\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mauthor_post_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], weights\u001b[38;5;241m=\u001b[39m[embedding_matrix],\n\u001b[1;32m     73\u001b[0m                   trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 100, 'weights': [array([[-0.24736658,  0.07031919,  0.01407981, ..., -0.27251738,\n         0.13224085,  0.0765908 ],\n       [-0.12291483,  0.08461943,  0.06060226, ..., -0.17144129,\n         0.17412944,  0.02834771],\n       [-0.15936373,  0.06352039,  0.02524688, ..., -0.229536  ,\n         0.12055779,  0.05851828],\n       ...,\n       [ 0.00078043,  0.02895817,  0.01444013, ..., -0.01905065,\n         0.02032049,  0.02145052],\n       [ 0.0083534 ,  0.05115167,  0.02525139, ..., -0.02803847,\n         0.03329531,  0.03750573],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])]}"
          ]
        }
      ],
      "source": [
        "cbow=1\n",
        "cbow_trainable=1\n",
        "sg=0 \n",
        "sg_trainable=0 \n",
        "data_split=2 \n",
        "batch=32 \n",
        "epochs=5\n",
        "\n",
        "acc_per_fold, precision_per_fold, recall_per_fold, f1_score_per_fold, avg_acc, avg_precision, avg_recall, avg_f1 = ModelDLWithWordEmbedding(cbow=1, cbow_trainable=1, sg=0, sg_trainable=0, data_split=2, batch=32, epochs=5, model_name=\"GRU\").train(posts, genders)\n",
        "print(\"acc_per_fold: \", acc_per_fold)\n",
        "print(\"precision_per_fold: \", precision_per_fold)\n",
        "print(\"recall_per_fold: \", recall_per_fold)\n",
        "print(\"f1_score_per_fold: \", f1_score_per_fold)\n",
        "print(\"avg_acc: \", avg_acc)\n",
        "print(\"avg_precision: \", avg_precision)\n",
        "print(\"avg_recall: \", avg_recall)\n",
        "print(\"avg_f1: \", avg_f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOQynE1QWYjhGgoa00uUN6r",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "bangla-bert-base.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
